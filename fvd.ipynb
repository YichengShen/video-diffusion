{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from scipy.linalg import sqrtm\n",
    "import numpy as np\n",
    "\n",
    "# https://huggingface.co/spaces/LanguageBind/Open-Sora-Plan-v1.0.0/blob/b1ca112023d762ebab42c48a0d70254ec95b2e4d/opensora/eval/fvd/styleganv/fvd.py\n",
    "\n",
    "device=torch.device('cpu')\n",
    "\n",
    "def load_i3d_pretrained(device=torch.device('cpu')):\n",
    "    i3D_WEIGHTS_URL = \"https://www.dropbox.com/s/ge9e5ujwgetktms/i3d_torchscript.pt\"\n",
    "    filepath = 'i3d_torchscript.pt'\n",
    "    print(filepath)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"preparing for download {i3D_WEIGHTS_URL}, you can download it by yourself.\")\n",
    "        os.system(f\"wget {i3D_WEIGHTS_URL} -O {filepath}\")\n",
    "    i3d = torch.jit.load(filepath).eval().to(device)\n",
    "    i3d = torch.nn.DataParallel(i3d)\n",
    "    return i3d\n",
    "\n",
    "def get_feats(videos, detector, device, bs=10):\n",
    "    # videos : torch.tensor BCTHW [0, 1]\n",
    "    detector_kwargs = dict(rescale=False, resize=False, return_features=True) # Return raw features before the softmax layer.\n",
    "    feats = np.empty((0, 400))\n",
    "    with torch.no_grad():\n",
    "        for i in range((len(videos)-1)//bs + 1):\n",
    "            feats = np.vstack([feats, detector(torch.stack([preprocess_single(video) for video in videos[i*bs:(i+1)*bs]]).to(device), **detector_kwargs).detach().cpu().numpy()])\n",
    "    return feats\n",
    "\n",
    "\n",
    "def get_fvd_feats(videos, i3d, device, bs=10):\n",
    "    # videos in [0, 1] as torch tensor BCTHW\n",
    "    # videos = [preprocess_single(video) for video in videos]\n",
    "    embeddings = get_feats(videos, i3d, device, bs)\n",
    "    return embeddings\n",
    "\n",
    "def preprocess_single(video, resolution=224, sequence_length=None):\n",
    "    # video: CTHW, [0, 1]\n",
    "    c, t, h, w = video.shape\n",
    "\n",
    "    # temporal crop\n",
    "    if sequence_length is not None:\n",
    "        assert sequence_length <= t\n",
    "        video = video[:, :sequence_length]\n",
    "\n",
    "    # scale shorter side to resolution\n",
    "    scale = resolution / min(h, w)\n",
    "    if h < w:\n",
    "        target_size = (resolution, math.ceil(w * scale))\n",
    "    else:\n",
    "        target_size = (math.ceil(h * scale), resolution)\n",
    "    video = F.interpolate(video, size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "    # center crop\n",
    "    c, t, h, w = video.shape\n",
    "    w_start = (w - resolution) // 2\n",
    "    h_start = (h - resolution) // 2\n",
    "    video = video[:, :, h_start:h_start + resolution, w_start:w_start + resolution]\n",
    "\n",
    "    # [0, 1] -> [-1, 1]\n",
    "    video = (video - 0.5) * 2\n",
    "\n",
    "    return video.contiguous()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Copy-pasted from https://github.com/cvpr2022-stylegan-v/stylegan-v/blob/main/src/metrics/frechet_video_distance.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compute_stats(feats: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    mu = feats.mean(axis=0) # [d]\n",
    "    sigma = np.cov(feats, rowvar=False) # [d, d]\n",
    "    return mu, sigma\n",
    "\n",
    "def frechet_distance(feats_fake: np.ndarray, feats_real: np.ndarray) -> float:\n",
    "    mu_gen, sigma_gen = compute_stats(feats_fake)\n",
    "    mu_real, sigma_real = compute_stats(feats_real)\n",
    "    m = np.square(mu_gen - mu_real).sum()\n",
    "    if feats_fake.shape[0] > 1:\n",
    "        s, _ = sqrtm(np.dot(sigma_gen, sigma_real), disp=False) # pylint: disable=no-member\n",
    "        fid = np.real(m + np.trace(sigma_gen + sigma_real - s * 2))\n",
    "    else:\n",
    "        fid = np.real(m)\n",
    "    return float(fid)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_our_fvd(videos_fake: torch.tensor, videos_real: torch.tensor, detector, device: str='cuda') -> float:\n",
    "    feats_fake = get_fvd_feats(videos_fake, detector, device, bs=2)\n",
    "    feats_real = get_fvd_feats(videos_real, detector, device, bs=2)\n",
    "\n",
    "    return frechet_distance(feats_fake, feats_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def video_to_tensor(filepath, size=(128, 128), len_frame=600, start_frame =0):\n",
    "    \"\"\"\n",
    "    Converts a video file into a PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "    filepath (str): Path to the MP4 video file.\n",
    "    size (tuple): The desired (height, width) to resize each frame.\n",
    "    len_frame (int): The number of frames to include in the tensor (video length).\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Tensor of shape (1, channels, len_frame, height, width)\n",
    "    \"\"\"\n",
    "    # Initialize a VideoCapture object to read video data from a file\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "    frames = []\n",
    "    \n",
    "    # Frame transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size),  # Resize each frame\n",
    "        transforms.ToTensor(),    # Convert image to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard normalization\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Read frames until you have enough or the video ends\n",
    "    count = 0\n",
    "    while len(frames) < len_frame:\n",
    "        ret, frame = cap.read()\n",
    "        if count >= start_frame:\n",
    "            if not ret:\n",
    "                print(count)\n",
    "                break  # Break the loop if there are no frames to read\n",
    "            # Convert color from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Apply transformations\n",
    "            frame = transform(frame)\n",
    "            frames.append(frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Stack frames along a new dimension (time dimension)\n",
    "    if len(frames) > 0:\n",
    "        video_tensor = torch.stack(frames)\n",
    "        # Add a batch dimension\n",
    "        video_tensor = video_tensor.unsqueeze(0)  # Shape: (1, channels, len_frame, height, width)\n",
    "        return video_tensor\n",
    "    else:\n",
    "        return torch.empty(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i3d_torchscript.pt\n"
     ]
    }
   ],
   "source": [
    "i3d = load_i3d_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7644517032287588"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = torch.rand(1,3,600, 128,128)\n",
    "video1 = torch.rand(100,3,600, 128,128)\n",
    "compute_our_fvd(video,video1,i3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it on our model inference output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hard = torch.load('data/mmnist-hard/batch_0_frames.pt', weights_only=True).permute(0,2,1,3,4).repeat(1, 3, 1, 1, 1)\n",
    "dataset_medium = torch.load('data/mmnist-medium/batch_0_frames.pt', weights_only=True).permute(0,2,1,3,4).repeat(1, 3, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_our_fvd(torch.rand(1,3,60, 72,128), dataset_hard, i3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Tensor shape: torch.Size([1, 3, 50, 72, 128])\n"
     ]
    }
   ],
   "source": [
    "filepath = 'video_hard_autoregressive.mp4'\n",
    "video_tensor = video_to_tensor(filepath, size=(72, 128), len_frame=100, start_frame=50).permute(0,2,1,3,4)\n",
    "print(\"Tensor shape:\", video_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14174.336900809723"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_our_fvd(video_tensor, dataset_hard, i3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([1, 3, 100, 72, 128])\n"
     ]
    }
   ],
   "source": [
    "filepath = 'medium_key.mp4'\n",
    "video_tensor = video_to_tensor(filepath, size=(72, 128), len_frame=100).permute(0,2,1,3,4)\n",
    "print(\"Tensor shape:\", video_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41003.24953633075"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_our_fvd(video_tensor, dataset_medium, i3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([1, 3, 100, 72, 128])\n"
     ]
    }
   ],
   "source": [
    "filepath = 'medium_pure_autoregressive.mp4'\n",
    "video_tensor = video_to_tensor(filepath, size=(72, 128), len_frame=100, start_frame=50).permute(0,2,1,3,4)\n",
    "print(\"Tensor shape:\", video_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21309.833642988368"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_our_fvd(video_tensor, dataset_medium, i3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "Tensor shape: torch.Size([1, 3, 87, 72, 128])\n"
     ]
    }
   ],
   "source": [
    "filepath = 'video.mp4'\n",
    "video_tensor = video_to_tensor(filepath, size=(72, 128), len_frame=100).permute(0,2,1,3,4)\n",
    "print(\"Tensor shape:\", video_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18096.241959897998"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_our_fvd(video_tensor, dataset_hard, i3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
